{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import gc\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # If CUDA is available, use a CUDA device\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    # If CUDA is not available, use the CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_train.csv',names= ['English','Hindi'],header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pd.read_csv('/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_test.csv',names = ['English','Hindi'],header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valdata = pd.read_csv('/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_valid.csv',names = ['English','Hindi'],header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(x)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eng_len = 0\n",
    "max_hin_len = 0\n",
    "test_max_eng_len = 0\n",
    "test_max_hin_len = 0\n",
    "val_max_eng_len = 0\n",
    "val_max_hin_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(testdata)):\n",
    "    temp = 0\n",
    "    for y in testdata.iloc[x]['English']:\n",
    "        temp+=1\n",
    "    test_max_eng_len = max(test_max_eng_len,temp)\n",
    "print(test_max_eng_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(testdata)):\n",
    "    temp = 0\n",
    "    for y in testdata.iloc[x]['Hindi']:\n",
    "        temp +=1\n",
    "    test_max_hin_len = max(test_max_hin_len,temp)\n",
    "print(test_max_hin_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(valdata)):\n",
    "    temp = 0\n",
    "    for y in valdata.iloc[x]['English']:\n",
    "        temp+=1\n",
    "    val_max_eng_len = max(val_max_eng_len,temp)\n",
    "print(val_max_eng_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "English_vocab = []\n",
    "for x in range(len(traindata)):\n",
    "    temp = 0\n",
    "    for y in traindata.iloc[x]['English']:\n",
    "        temp += 1\n",
    "        if y not in English_vocab:\n",
    "            English_vocab.append(y)\n",
    "    if(temp>max_eng_len):\n",
    "        max_eng_len = max(max_eng_len,temp)\n",
    "print(sorted(English_vocab))\n",
    "print(max_eng_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hindi_vocab = []\n",
    "for x in range(len(traindata)):\n",
    "    temp = 0\n",
    "    for y in traindata.iloc[x]['Hindi']:\n",
    "        temp += 1\n",
    "        if y not in Hindi_vocab:\n",
    "            Hindi_vocab.append(y)\n",
    "    max_hin_len = max(temp,max_hin_len)\n",
    "for x in range(len(testdata)):\n",
    "    for y in testdata.iloc[x]['Hindi']:\n",
    "        if y not in Hindi_vocab:\n",
    "            print(y)\n",
    "            Hindi_vocab.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "English_vocab = sorted(English_vocab)\n",
    "Hindi_vocab = sorted(Hindi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eng_dict = {}\n",
    "reverse_Eng = {}\n",
    "\n",
    "for x in range(len(English_vocab)):\n",
    "    Eng_dict[English_vocab[x]] = x+3\n",
    "    reverse_Eng[x+3] = English_vocab[x]\n",
    "Eng_dict['<sow>'] = 0\n",
    "Eng_dict['<eow>'] = 1\n",
    "Eng_dict['<pad>'] = 2\n",
    "reverse_Eng[0] = '<sow>'\n",
    "reverse_Eng[1] = '<eow>'\n",
    "reverse_Eng[2] = '<pad>'\n",
    "\n",
    "print(Eng_dict)\n",
    "print(reverse_Eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hin_dict = {}\n",
    "reverse_Hin = {}\n",
    "for x in range(len(Hindi_vocab)):\n",
    "    Hin_dict[Hindi_vocab[x]] = x+3\n",
    "    reverse_Hin[x+3] = Hindi_vocab[x]\n",
    "Hin_dict['<sow>'] = 0\n",
    "Hin_dict['<eow>'] = 1\n",
    "Hin_dict['<pad>'] = 2\n",
    "reverse_Hin[0] = '<sow>'\n",
    "reverse_Hin[1] = '<eow>'\n",
    "reverse_Hin[2] = '<pad>'\n",
    "print(Hin_dict)\n",
    "print(reverse_Hin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eng_tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(Eng_dict[x])\n",
    "    for x in range(len(tokens),max_eng_len):\n",
    "        tokens.append(Eng_dict['<pad>'])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hin_tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(Hin_dict[x])\n",
    "    tokens.append(Hin_dict['<eow>'])\n",
    "    for x in range(len(tokens),max_hin_len+1):\n",
    "        tokens.append(Hin_dict['<pad>'])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_word = []\n",
    "hin_word = []\n",
    "for x in range(len(traindata)):\n",
    "    eng_word.append(Eng_tokenize(traindata.iloc[x]['English']))\n",
    "    hin_word.append(Hin_tokenize(traindata.iloc[x]['Hindi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_word = torch.tensor(eng_word)\n",
    "hin_word = torch.tensor(hin_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_hin_len += 1\n",
    "test_max_hin_len += 1\n",
    "val_max_hin_len += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Eng_tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(Eng_dict[x])\n",
    "    for x in range(len(tokens),test_max_eng_len):\n",
    "        tokens.append(Eng_dict['<pad>'])\n",
    "    return tokens\n",
    "def test_Hin_tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(Hin_dict[x])\n",
    "    tokens.append(Hin_dict['<eow>'])\n",
    "    for x in range(len(tokens),test_max_hin_len):\n",
    "        tokens.append(Hin_dict['<pad>'])\n",
    "    return tokens\n",
    "def val_Eng_tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(Eng_dict[x])\n",
    "    for x in range(len(tokens),val_max_eng_len):\n",
    "        tokens.append(Eng_dict['<pad>'])\n",
    "    return tokens\n",
    "def val_Hin_tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(Hin_dict[x])\n",
    "    tokens.append(Hin_dict['<eow>'])\n",
    "    for x in range(len(tokens),val_max_hin_len):\n",
    "        tokens.append(Hin_dict['<pad>'])\n",
    "    return tokens\n",
    "val_eng_word = []\n",
    "val_hin_word = []\n",
    "for x in range(len(valdata)):\n",
    "    val_eng_word.append(val_Eng_tokenize(valdata.iloc[x]['English']))\n",
    "    val_hin_word.append(val_Hin_tokenize(valdata.iloc[x]['Hindi']))\n",
    "val_eng_word = torch.tensor(val_eng_word)\n",
    "val_hin_word = torch.tensor(val_hin_word)\n",
    "test_eng_word = []\n",
    "test_hin_word = []\n",
    "for x in range(len(testdata)):\n",
    "    test_eng_word.append(test_Eng_tokenize(testdata.iloc[x]['English']))\n",
    "    test_hin_word.append(test_Hin_tokenize(testdata.iloc[x]['Hindi']))\n",
    "test_eng_word = torch.tensor(test_eng_word)\n",
    "test_hin_word = torch.tensor(test_hin_word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,char_embed_size,hidden_size,no_of_layers,dropout,rnn):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.layer = no_of_layers\n",
    "        self.rnn = rnn\n",
    "        self.embedding = nn.Embedding(len(Eng_dict),char_embed_size).to(device)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.LSTM = nn.LSTM(char_embed_size,hidden_size,self.layer,batch_first = True,bidirectional = True).to(device)\n",
    "        self.RNN = nn.RNN(char_embed_size,hidden_size,self.layer,batch_first = True,bidirectional = True).to(device)\n",
    "        self.GRU = nn.GRU(char_embed_size,hidden_size,self.layer,batch_first = True,bidirectional = True).to(device)\n",
    "    def forward(self,input,hidden,cell):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded1 = self.drop(embedded)\n",
    "        cell1 = cell\n",
    "        if(self.rnn == 'RNN'):\n",
    "            output,hidden1 = self.RNN(embedded1,hidden)\n",
    "        elif(self.rnn == 'LSTM'):\n",
    "            output,(hidden1,cell1) = self.LSTM(embedded1,(hidden,cell))\n",
    "        elif(self.rnn == 'GRU'):\n",
    "            output,hidden1 = self.GRU(embedded1,hidden)\n",
    "        return output,(hidden1,cell1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,char_embed_size,hidden_size,no_of_layers,dropout,batchsize,rnn):\n",
    "        super(Attention,self).__init__()\n",
    "        self.layer = no_of_layers\n",
    "        self.batchsize = batchsize\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = rnn\n",
    "        self.embedding = nn.Embedding(len(Hin_dict),char_embed_size).to(device)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.U = nn.Linear(hidden_size,hidden_size,bias = False).to(device)\n",
    "        self.W = nn.Linear(hidden_size,hidden_size,bias = False).to(device)\n",
    "        self.V = nn.Linear(hidden_size,1,bias = False).to(device)\n",
    "        self.LSTM = nn.LSTM(char_embed_size + hidden_size,hidden_size,self.layer,batch_first = True).to(device)\n",
    "        self.RNN = nn.RNN(char_embed_size + hidden_size,hidden_size,self.layer,batch_first = True).to(device)\n",
    "        self.GRU = nn.GRU(char_embed_size + hidden_size,hidden_size,self.layer,batch_first = True).to(device) \n",
    "        self.linear = nn.Linear(hidden_size,len(Hin_dict),bias=True).to(device)\n",
    "        self.softmax = nn.Softmax(dim = 2).to(device)\n",
    "    def forward(self,input,hidden,cell,encoder_outputs,matrix):\n",
    "        embedded = self.embedding(input)\n",
    "        temp1 = self.U(encoder_outputs)\n",
    "        temp2 = self.W(hidden[-1])\n",
    "        s1 = temp2.size()[0]\n",
    "        s2 = temp2.size()[1]\n",
    "        add = temp1 + temp2.resize(s1,1,s2)\n",
    "        tanh = F.tanh(add)\n",
    "        ejt = self.V(tanh)\n",
    "        ajt = nn.Softmax(dim = 1)(ejt)\n",
    "        ct = torch.zeros(self.batchsize,1,self.hidden_size).to(device)\n",
    "        ct = torch.bmm(ajt.transpose(1,2),encoder_outputs)\n",
    "        final_input = torch.cat((embedded,ct),dim = 2)\n",
    "        final_input = self.drop(final_input)\n",
    "        cell1 = cell\n",
    "        if(self.rnn == 'LSTM'):\n",
    "            output,(hidden1,cell1) = self.LSTM(final_input,(hidden,cell))\n",
    "        elif(self.rnn == 'RNN'):\n",
    "            output,hidden1 = self.RNN(final_input,hidden)\n",
    "        elif(self.rnn == 'GRU'):\n",
    "            output,hidden1 = self.GRU(final_input,hidden)\n",
    "        output1 = self.linear(output)\n",
    "        if(matrix == True):\n",
    "            return ajt,output1,(hidden1,cell1)\n",
    "        return output1,(hidden1,cell1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getword(characters):\n",
    "    return \"\".join(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(target,predictions,flag):\n",
    "    total = 0\n",
    "    for x in range(len(target)):\n",
    "        if(torch.equal(target[x],predictions[x])):\n",
    "            total += 1\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(target,predictions,df):\n",
    "    i = len(df)\n",
    "    for x in range(len(predictions)):\n",
    "        original = []\n",
    "        for y in target[x]:\n",
    "            if(y != 1):\n",
    "                original.append(y)\n",
    "            else:\n",
    "                break\n",
    "        predicted = []\n",
    "        for y in predictions[x]:\n",
    "            if(y != 1):\n",
    "                predicted.append(y)\n",
    "            else:\n",
    "                break\n",
    "        df.loc[i,['Original']] = getword([reverse_Hin[x.item()] for x in original])\n",
    "        df.loc[i,['Predicted']] = getword([reverse_Hin[x.item()] for x in predicted])\n",
    "        i+=1\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(attention,test_eng_word,test_hin_word,encoder,decoder,batchsize,hidden_size,char_embed_size,no_of_layers):\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        df = pd.DataFrame()\n",
    "        en_hidden = torch.zeros(2*no_of_layers,batchsize,hidden_size).to(device)\n",
    "        en_cell = torch.zeros(2*no_of_layers,batchsize,hidden_size).to(device)\n",
    "        for x in range(0,len(testdata),batchsize):\n",
    "            loss = 0\n",
    "            input_tensor = test_eng_word[x:x+batchsize].to(device)\n",
    "            if(input_tensor.size()[0] < batchsize):\n",
    "                break\n",
    "            output,(hidden,cell) = encoder.forward(input_tensor,en_hidden,en_cell)\n",
    "            del(input_tensor)\n",
    "            output = torch.split(output,[hidden_size,hidden_size],dim = 2)\n",
    "            output = torch.add(output[0],output[1])/2\n",
    "            input2 = []\n",
    "            for y in range(batchsize):\n",
    "                input2.append([0])\n",
    "            input2 = torch.tensor(input2).to(device)\n",
    "            hidden = hidden.resize(2,no_of_layers,batchsize,hidden_size)\n",
    "            hidden1 = torch.add(hidden[0],hidden[1])/2\n",
    "            cell = cell.resize(2,no_of_layers,batchsize,hidden_size)\n",
    "            cell1 = torch.add(cell[0],cell[1])/2\n",
    "            OGhidden = hidden1\n",
    "            predicted = []\n",
    "            predictions = []\n",
    "            if(attention == True):\n",
    "                temp = output\n",
    "            else:\n",
    "                temp = OGhidden\n",
    "            for i in range(test_max_hin_len):\n",
    "                output1,(hidden1,cell1) = decoder.forward(input2,hidden1,cell1,temp,False)\n",
    "                predicted.append(output1)\n",
    "                output2 = decoder.softmax(output1)\n",
    "                output3 = torch.argmax(output2,dim = 2)\n",
    "                predictions.append(output3)\n",
    "                input2 = output3\n",
    "            predicted = torch.cat(tuple(x for x in predicted),dim =1).to(device).resize(test_max_hin_len*batchsize,len(Hin_dict))\n",
    "            predictions = torch.cat(tuple(x for x in predictions),dim =1).to(device)\n",
    "            total_acc += accuracy(test_hin_word[x:x+batchsize].to(device),predictions,x)\n",
    "            df = translate(test_hin_word[x:x+batchsize],predictions,df)\n",
    "            loss  = nn.CrossEntropyLoss(reduction = 'sum')(predicted,test_hin_word[x:x+batchsize].reshape(-1).to(device))\n",
    "            with torch.no_grad():\n",
    "                total_loss += loss.item()\n",
    "        test_loss = total_loss/(len(testdata)*test_max_hin_len)\n",
    "        test_accuracy = (total_acc/len(testdata))*100\n",
    "        del(predictions)\n",
    "        del(predicted)\n",
    "        del(input2)\n",
    "        del(output1)\n",
    "        del(output2)\n",
    "        del(output3)\n",
    "        del(hidden1)\n",
    "        del(cell1)\n",
    "        del(OGhidden)\n",
    "        del(output)\n",
    "        del(cell)\n",
    "        return test_loss,test_accuracy,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valevaluate(attention,val_eng_word,val_hin_word,encoder,decoder,batchsize,hidden_size,char_embed_size,no_of_layers):\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for x in range(0,len(valdata),batchsize):\n",
    "            loss = 0\n",
    "            input_tensor = val_eng_word[x:x+batchsize].to(device)\n",
    "#             en_hidden = torch.zeros(2*no_of_layers,batchsize,hidden_size).to(device)\n",
    "            if(input_tensor.size()[0] < batchsize):\n",
    "                break\n",
    "            en_hidden = torch.zeros(2*no_of_layers,batchsize,hidden_size).to(device)\n",
    "            en_cell = torch.zeros(2*no_of_layers,batchsize,hidden_size).to(device)\n",
    "            output,(hidden,cell) = encoder.forward(input_tensor,en_hidden,en_cell)\n",
    "            del(input_tensor)\n",
    "            del(en_hidden)\n",
    "            del(en_cell)\n",
    "            output = torch.split(output,[hidden_size,hidden_size],dim = 2)\n",
    "            output = torch.add(output[0],output[1])/2\n",
    "            input2 = []\n",
    "            for y in range(batchsize):\n",
    "                input2.append([0])\n",
    "            input2 = torch.tensor(input2).to(device)\n",
    "            hidden = hidden.resize(2,no_of_layers,batchsize,hidden_size)\n",
    "            hidden1 = torch.add(hidden[0],hidden[1])/2\n",
    "#             hidden1 = hidden[0]\n",
    "            cell = cell.resize(2,no_of_layers,batchsize,hidden_size)\n",
    "            cell1 = torch.add(cell[0],cell[1])/2\n",
    "#             cell1 = cell[0]\n",
    "            OGhidden = hidden1\n",
    "            predicted = []\n",
    "            predictions = []\n",
    "            if(attention == True):\n",
    "                temp = output\n",
    "            else:\n",
    "                temp = OGhidden\n",
    "            for i in range(val_max_hin_len):\n",
    "                output1,(hidden1,cell1) = decoder.forward(input2,hidden1,cell1,temp,False)\n",
    "                predicted.append(output1)\n",
    "                output2 = decoder.softmax(output1)\n",
    "                output3 = torch.argmax(output2,dim = 2)\n",
    "                predictions.append(output3)\n",
    "                input2 = output3\n",
    "            predicted = torch.cat(tuple(x for x in predicted),dim =1).to(device).resize(val_max_hin_len*batchsize,len(Hin_dict))\n",
    "            predictions = torch.cat(tuple(x for x in predictions),dim =1).to(device)\n",
    "            total_acc += accuracy(val_hin_word[x:x+batchsize].to(device),predictions,x)\n",
    "            loss  = nn.CrossEntropyLoss(reduction = 'sum')(predicted,val_hin_word[x:x+batchsize].reshape(-1).to(device))\n",
    "            with torch.no_grad():\n",
    "                total_loss += loss.item()\n",
    "#             print(loss.item())\n",
    "        validation_loss = total_loss/(len(valdata)*val_max_hin_len)\n",
    "        validation_accuracy = (total_acc/len(valdata))*100\n",
    "        del(predictions)\n",
    "        del(predicted)\n",
    "        del(input2)\n",
    "        del(output1)\n",
    "        del(output2)\n",
    "        del(output3)\n",
    "        del(hidden1)\n",
    "        del(cell1)\n",
    "        del(OGhidden)\n",
    "        del(output)\n",
    "        del(cell)\n",
    "        return validation_loss,validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentiontrain(batchsize,hidden_size,char_embed_size,no_of_layers,dropout,epochs,rnn):\n",
    "    gc.collect()\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    encoder = Encoder(char_embed_size,hidden_size,no_of_layers,dropout,rnn).to(device)\n",
    "    decoder = Attention(char_embed_size,hidden_size,no_of_layers,dropout,batchsize,rnn).to(device)\n",
    "    print(encoder.parameters)\n",
    "    print(decoder.parameters)\n",
    "    opt_encoder = optim.Adam(encoder.parameters(),lr = 0.001)\n",
    "    opt_decoder  = optim.Adam(decoder.parameters(),lr = 0.001)\n",
    "    teacher_ratio = 0.5\n",
    "    for _ in range(epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(_)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for x in range(0,len(traindata),batchsize):\n",
    "            loss = 0\n",
    "            opt_encoder.zero_grad()\n",
    "            opt_decoder.zero_grad()\n",
    "            input_tensor = eng_word[x:x+batchsize].to(device)\n",
    "            en_hidden = torch.zeros(2*no_of_layers,batchsize,hidden_size).to(device)\n",
    "            en_cell = torch.zeros(2*no_of_layers,batchsize,hidden_size).to(device)\n",
    "            if(input_tensor.size()[0] < batchsize):\n",
    "                break\n",
    "            output,(hidden,cell) = encoder.forward(input_tensor,en_hidden,en_cell)\n",
    "            output = torch.split(output,[hidden_size,hidden_size],dim = 2)\n",
    "            output = torch.add(output[0],output[1])/2\n",
    "            input2 = []\n",
    "            for y in range(batchsize):\n",
    "                input2.append([0])\n",
    "            input2 = torch.tensor(input2).to(device)\n",
    "            hidden = hidden.resize(2,no_of_layers,batchsize,hidden_size)\n",
    "            hidden1 = torch.add(hidden[0],hidden[1])/2\n",
    "            cell = cell.resize(2,no_of_layers,batchsize,hidden_size)\n",
    "            cell1 = torch.add(cell[0],cell[1])/2\n",
    "            predicted = []\n",
    "            predictions = []\n",
    "#             use_teacher_forcing = True if random.random() < teacher_ratio else False\n",
    "            for i in range(max_hin_len):\n",
    "                use_teacher_forcing = True if random.random() < teacher_ratio else False\n",
    "                output1,(hidden1,cell1) = decoder.forward(input2,hidden1,cell1,output,False)\n",
    "                predicted.append(output1)\n",
    "                output2 = decoder.softmax(output1)\n",
    "                output3 = torch.argmax(output2,dim = 2)\n",
    "                predictions.append(output3)\n",
    "                if(use_teacher_forcing):\n",
    "                    input2 = hin_word[x:x+batchsize,i].to(device).resize(batchsize,1)\n",
    "                else:\n",
    "                    input2 = hin_word[x:x+batchsize,i].to(device).resize(batchsize,1)\n",
    "            \n",
    "            predicted = torch.cat(tuple(x for x in predicted),dim =1).to(device).resize(max_hin_len*batchsize,len(Hin_dict))\n",
    "            predictions = torch.cat(tuple(x for x in predictions),dim =1).to(device)\n",
    "            total_acc += accuracy(hin_word[x:x+batchsize].to(device),predictions,x)\n",
    "            loss  = nn.CrossEntropyLoss(reduction = 'sum')(predicted,hin_word[x:x+batchsize].reshape(-1).to(device))\n",
    "            with torch.no_grad():\n",
    "                total_loss += loss.item()\n",
    "            loss.backward(retain_graph = True)\n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(),max_norm = 1)\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(),max_norm = 1)\n",
    "            opt_encoder.step()\n",
    "            opt_decoder.step()\n",
    "        del(input_tensor)\n",
    "        del(en_hidden)\n",
    "        del(en_cell)\n",
    "        del(predictions)\n",
    "        del(predicted)\n",
    "        del(input2)\n",
    "        del(output1)\n",
    "        del(output2)\n",
    "        del(output3)\n",
    "        del(hidden)\n",
    "        del(hidden1)\n",
    "        del(cell1)\n",
    "        del(output)\n",
    "        del(cell)\n",
    "        training_loss = total_loss/(51200*max_hin_len)\n",
    "        training_accuracy = total_acc/512\n",
    "        validation_loss,validation_accuracy = valevaluate(True,val_eng_word,val_hin_word,encoder,decoder,batchsize,hidden_size,char_embed_size,no_of_layers)\n",
    "#         wandb.log({'training_accuracy' : training_accuracy, 'validation_accuracy' : validation_accuracy,'training_loss' : training_loss, 'validation_loss' : validation_loss,'epoch':_+1})\n",
    "#         if(_ >= epochs/2):\n",
    "#             teacher_ratio = 0\n",
    "#         teacher_ratio /= 2\n",
    "    return encoder,decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GPUtil in /opt/homebrew/lib/python3.10/site-packages (1.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install GPUtil\n",
    "\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    wandb.init(project='CS6910_DLAssignment3')\n",
    "    config = wandb.config\n",
    "    wandb.run.name = \"_attention_cell_type_{}_bidirec_{}_layers_{}_batchsize_{}_hidden_{}\".format(config.cell_type,config.bidirectional,config.no_of_layers,config.batchsize,config.hidden_size)\n",
    "    hidden_size = config.hidden_size\n",
    "    char_embed_size = config.input_embedding_size\n",
    "    no_of_layers = config.no_of_layers\n",
    "    epochs = config.epochs\n",
    "    batchsize = config.batchsize\n",
    "    dropout = config.dropout\n",
    "    rnn = config.cell_type\n",
    "    Encoder1,Decoder1 = attentiontrain(batchsize,hidden_size,char_embed_size,no_of_layers,dropout,epochs,rnn)\n",
    "    attentionmatrix(True,test_eng_word,test_hin_word,Encoder1,Decoder1,1,hidden_size,char_embed_size,no_of_layers,True)\n",
    "    free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    'method' : 'bayes',\n",
    "    'metric' : { 'goal' : 'maximize',\n",
    "    'name' : 'validation_accuracy'},\n",
    "    'parameters':{\n",
    "        'batchsize' : {'values' : [128,256,512]},\n",
    "        'input_embedding_size' : {'values' : [128,256,512,1024]},\n",
    "        'no_of_layers' : {'values' : [1,2,3,4]},\n",
    "        'hidden_size' : {'values' : [128,256,512,1024]},\n",
    "        'cell_type' : {'values' : ['LSTM']},\n",
    "        'bidirectional' : {'values' : ['Yes']},\n",
    "        'dropout' : {'values' : [0.3,0.4,0.5]},\n",
    "        'epochs' : {'values' : [10,20,30]}\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep = sweep_configuration,project = 'CS6910_DLAssignment3')\n",
    "wandb.agent(sweep_id,function=main,count = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best parameters for attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder1,Decoder1 = attentiontrain(batchsize,hidden_size,char_embed_size,no_of_layers,dropout,epochs,rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_accuracy,predictions = Evaluate(True,test_eng_word,test_hin_word,Encoder1,Decoder1,batchsize,hidden_size,char_embed_size,no_of_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine predictions and english column of testdata and show just head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_excel(\"predictions_vanilla/output.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
