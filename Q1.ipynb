{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import gc\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # If CUDA is available, use a CUDA device\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    # If CUDA is not available, use the CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_train.csv',names= ['English','Hindi'],header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pd.read_csv('/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_test.csv',names = ['English','Hindi'],header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valdata = pd.read_csv('/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_valid.csv',names = ['English','Hindi'],header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(x)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eng_len = 0\n",
    "max_hin_len = 0\n",
    "test_max_eng_len = 0\n",
    "test_max_hin_len = 0\n",
    "val_max_eng_len = 0\n",
    "val_max_hin_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(testdata)):\n",
    "    temp = 0\n",
    "    for y in testdata.iloc[x]['English']:\n",
    "        temp+=1\n",
    "    test_max_eng_len = max(test_max_eng_len,temp)\n",
    "print(test_max_eng_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(testdata)):\n",
    "    temp = 0\n",
    "    for y in testdata.iloc[x]['Hindi']:\n",
    "        temp +=1\n",
    "    test_max_hin_len = max(test_max_hin_len,temp)\n",
    "print(test_max_hin_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(valdata)):\n",
    "    temp = 0\n",
    "    for y in valdata.iloc[x]['English']:\n",
    "        temp+=1\n",
    "    val_max_eng_len = max(val_max_eng_len,temp)\n",
    "print(val_max_eng_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(valdata)):\n",
    "    temp = 0\n",
    "    for y in valdata.iloc[x]['Hindi']:\n",
    "        temp+=1\n",
    "    val_max_hin_len = max(val_max_hin_len,temp)\n",
    "print(val_max_hin_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "English_vocab = []\n",
    "for x in range(len(traindata)):\n",
    "    temp = 0\n",
    "    for y in traindata.iloc[x]['English']:\n",
    "        temp += 1\n",
    "        if y not in English_vocab:\n",
    "            English_vocab.append(y)\n",
    "    if(temp>max_eng_len):\n",
    "        max_eng_len = max(max_eng_len,temp)\n",
    "print(sorted(English_vocab))\n",
    "print(max_eng_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hindi_vocab = []\n",
    "for x in range(len(traindata)):\n",
    "    temp = 0\n",
    "    for y in traindata.iloc[x]['Hindi']:\n",
    "        temp += 1\n",
    "        if y not in Hindi_vocab:\n",
    "            Hindi_vocab.append(y)\n",
    "    max_hin_len = max(temp,max_hin_len)\n",
    "for x in range(len(testdata)):\n",
    "    for y in testdata.iloc[x]['Hindi']:\n",
    "        if y not in Hindi_vocab:\n",
    "            print(y)\n",
    "            Hindi_vocab.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "English_vocab = sorted(English_vocab)\n",
    "Hindi_vocab = sorted(Hindi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eng_dict = {}\n",
    "reverse_Eng = {}\n",
    "\n",
    "for x in range(len(English_vocab)):\n",
    "    Eng_dict[English_vocab[x]] = x+3\n",
    "    reverse_Eng[x+3] = English_vocab[x]\n",
    "Eng_dict['<sow>'] = 0\n",
    "Eng_dict['<eow>'] = 1\n",
    "Eng_dict['<pad>'] = 2\n",
    "reverse_Eng[0] = '<sow>'\n",
    "reverse_Eng[1] = '<eow>'\n",
    "reverse_Eng[2] = '<pad>'\n",
    "\n",
    "print(Eng_dict)\n",
    "print(reverse_Eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hin_dict = {}\n",
    "reverse_Hin = {}\n",
    "for x in range(len(Hindi_vocab)):\n",
    "    Hin_dict[Hindi_vocab[x]] = x+3\n",
    "    reverse_Hin[x+3] = Hindi_vocab[x]\n",
    "Hin_dict['<sow>'] = 0\n",
    "Hin_dict['<eow>'] = 1\n",
    "Hin_dict['<pad>'] = 2\n",
    "reverse_Hin[0] = '<sow>'\n",
    "reverse_Hin[1] = '<eow>'\n",
    "reverse_Hin[2] = '<pad>'\n",
    "print(Hin_dict)\n",
    "print(reverse_Hin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eng_tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(Eng_dict[x])\n",
    "    for x in range(len(tokens),max_eng_len):\n",
    "        tokens.append(Eng_dict['<pad>'])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hin_tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(Hin_dict[x])\n",
    "    tokens.append(Hin_dict['<eow>'])\n",
    "    for x in range(len(tokens),max_hin_len+1):\n",
    "        tokens.append(Hin_dict['<pad>'])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_word = []\n",
    "hin_word = []\n",
    "for x in range(len(traindata)):\n",
    "    eng_word.append(Eng_tokenize(traindata.iloc[x]['English']))\n",
    "    hin_word.append(Hin_tokenize(traindata.iloc[x]['Hindi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_word = torch.tensor(eng_word)\n",
    "hin_word = torch.tensor(hin_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_hin_len += 1\n",
    "test_max_hin_len += 1\n",
    "val_max_hin_len += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Eng_tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(Eng_dict[x])\n",
    "    for x in range(len(tokens),test_max_eng_len):\n",
    "        tokens.append(Eng_dict['<pad>'])\n",
    "    return tokens\n",
    "def test_Hin_tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(Hin_dict[x])\n",
    "    tokens.append(Hin_dict['<eow>'])\n",
    "    for x in range(len(tokens),test_max_hin_len):\n",
    "        tokens.append(Hin_dict['<pad>'])\n",
    "    return tokens\n",
    "def val_Eng_tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(Eng_dict[x])\n",
    "    for x in range(len(tokens),val_max_eng_len):\n",
    "        tokens.append(Eng_dict['<pad>'])\n",
    "    return tokens\n",
    "def val_Hin_tokenize(word):\n",
    "    tokens = []\n",
    "    for x in word:\n",
    "        tokens.append(Hin_dict[x])\n",
    "    tokens.append(Hin_dict['<eow>'])\n",
    "    for x in range(len(tokens),val_max_hin_len):\n",
    "        tokens.append(Hin_dict['<pad>'])\n",
    "    return tokens\n",
    "val_eng_word = []\n",
    "val_hin_word = []\n",
    "for x in range(len(valdata)):\n",
    "    val_eng_word.append(val_Eng_tokenize(valdata.iloc[x]['English']))\n",
    "    val_hin_word.append(val_Hin_tokenize(valdata.iloc[x]['Hindi']))\n",
    "val_eng_word = torch.tensor(val_eng_word)\n",
    "val_hin_word = torch.tensor(val_hin_word)\n",
    "test_eng_word = []\n",
    "test_hin_word = []\n",
    "for x in range(len(testdata)):\n",
    "    test_eng_word.append(test_Eng_tokenize(testdata.iloc[x]['English']))\n",
    "    test_hin_word.append(test_Hin_tokenize(testdata.iloc[x]['Hindi']))\n",
    "test_eng_word = torch.tensor(test_eng_word)\n",
    "test_hin_word = torch.tensor(test_hin_word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,char_embed_size,hidden_size,no_of_layers,dropout,rnn):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.layer = no_of_layers\n",
    "        self.rnn = rnn\n",
    "        self.embedding = nn.Embedding(len(Eng_dict),char_embed_size).to(device)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.LSTM = nn.LSTM(char_embed_size,hidden_size,self.layer,batch_first = True,bidirectional = True).to(device)\n",
    "        self.RNN = nn.RNN(char_embed_size,hidden_size,self.layer,batch_first = True,bidirectional = True).to(device)\n",
    "        self.GRU = nn.GRU(char_embed_size,hidden_size,self.layer,batch_first = True,bidirectional = True).to(device)\n",
    "    def forward(self,input,hidden,cell):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded1 = self.drop(embedded)\n",
    "        cell1 = cell\n",
    "        if(self.rnn == 'RNN'):\n",
    "            output,hidden1 = self.RNN(embedded1,hidden)\n",
    "        elif(self.rnn == 'LSTM'):\n",
    "            output,(hidden1,cell1) = self.LSTM(embedded1,(hidden,cell))\n",
    "        elif(self.rnn == 'GRU'):\n",
    "            output,hidden1 = self.GRU(embedded1,hidden)\n",
    "        return output,(hidden1,cell1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,char_embed_size,hidden_size,no_of_layers,dropout,batchsize,rnn):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.layer = no_of_layers\n",
    "        self.batchsize = batchsize\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = rnn\n",
    "        self.embedding = nn.Embedding(len(Hin_dict),char_embed_size).to(device)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.LSTM = nn.LSTM(char_embed_size + hidden_size*2,hidden_size,self.layer,batch_first = True).to(device)\n",
    "        self.RNN = nn.RNN(char_embed_size + hidden_size*2,hidden_size,self.layer,batch_first = True).to(device)\n",
    "        self.GRU = nn.GRU(char_embed_size + hidden_size*2,hidden_size,self.layer,batch_first = True).to(device)\n",
    "        #2*hidden_size\n",
    "        self.linear = nn.Linear(hidden_size,len(Hin_dict),bias=True).to(device)\n",
    "        # dim = 2 \n",
    "        self.softmax = nn.Softmax(dim = 2).to(device)\n",
    "    def forward(self,input,hidden,cell,OGhidden,matrix):\n",
    "        embedded = self.embedding(input)\n",
    "        s1 = OGhidden.size()[1]\n",
    "        s2 = OGhidden.size()[2]\n",
    "        embedded1 = torch.cat((embedded,OGhidden[0].resize(s1,1,s2),OGhidden[1].resize(s1,1,s2)),dim = 2)\n",
    "        embedded2 = self.drop(embedded1)\n",
    "        cell1 = cell\n",
    "        if(self.rnn == 'LSTM'):\n",
    "            output,(hidden1,cell1) = self.LSTM(embedded2,(hidden,cell))\n",
    "        elif(self.rnn == 'RNN'):\n",
    "            output,hidden1 = self.RNN(embedded2,hidden)\n",
    "        elif(self.rnn == 'GRU'):\n",
    "            output,hidden1 = self.GRU(embedded2,hidden)\n",
    "        output1 = self.linear(output)\n",
    "        return output1,(hidden1,cell1)\n",
    "        output2 = self.softmax(output1)\n",
    "        return output2,hidden11\n",
    "        \n",
    "    #changed GRU char_embed_size\n",
    "    #changed forward embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getword(characters):\n",
    "    return \"\".join(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(target,predictions,flag):\n",
    "    total = 0\n",
    "    for x in range(len(target)):\n",
    "        if(torch.equal(target[x],predictions[x])):\n",
    "            total += 1\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(target,predictions,df):\n",
    "    i = len(df)\n",
    "    for x in range(len(predictions)):\n",
    "        original = []\n",
    "        for y in target[x]:\n",
    "            if(y != 1):\n",
    "                original.append(y)\n",
    "            else:\n",
    "                break\n",
    "        predicted = []\n",
    "        for y in predictions[x]:\n",
    "            if(y != 1):\n",
    "                predicted.append(y)\n",
    "            else:\n",
    "                break\n",
    "        df.loc[i,['Original']] = getword([reverse_Hin[x.item()] for x in original])\n",
    "        df.loc[i,['Predicted']] = getword([reverse_Hin[x.item()] for x in predicted])\n",
    "        i+=1\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Seq2Seq(attention,test_eng_word,test_hin_word,encoder,decoder,batchsize,hidden_size,char_embed_size,no_of_layers):\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        df = pd.DataFrame()\n",
    "        en_hidden = torch.zeros(2*no_of_layers,batchsize,hidden_size).to(device)\n",
    "        en_cell = torch.zeros(2*no_of_layers,batchsize,hidden_size).to(device)\n",
    "        for x in range(0,len(testdata),batchsize):\n",
    "            loss = 0\n",
    "            input_tensor = test_eng_word[x:x+batchsize].to(device)\n",
    "            if(input_tensor.size()[0] < batchsize):\n",
    "                break\n",
    "            output,(hidden,cell) = encoder.forward(input_tensor,en_hidden,en_cell)\n",
    "            del(input_tensor)\n",
    "            output = torch.split(output,[hidden_size,hidden_size],dim = 2)\n",
    "            output = torch.add(output[0],output[1])/2\n",
    "            input2 = []\n",
    "            for y in range(batchsize):\n",
    "                input2.append([0])\n",
    "            input2 = torch.tensor(input2).to(device)\n",
    "            hidden = hidden.resize(2,no_of_layers,batchsize,hidden_size)\n",
    "            hidden1 = torch.add(hidden[0],hidden[1])/2\n",
    "            cell = cell.resize(2,no_of_layers,batchsize,hidden_size)\n",
    "            cell1 = torch.add(cell[0],cell[1])/2\n",
    "            OGhidden = hidden1\n",
    "            predicted = []\n",
    "            predictions = []\n",
    "            if(attention == True):\n",
    "                temp = output\n",
    "            else:\n",
    "                temp = OGhidden\n",
    "            for i in range(test_max_hin_len):\n",
    "                output1,(hidden1,cell1) = decoder.forward(input2,hidden1,cell1,temp,False)\n",
    "                predicted.append(output1)\n",
    "                output2 = decoder.softmax(output1)\n",
    "                output3 = torch.argmax(output2,dim = 2)\n",
    "                predictions.append(output3)\n",
    "                input2 = output3\n",
    "            predicted = torch.cat(tuple(x for x in predicted),dim =1).to(device).resize(test_max_hin_len*batchsize,len(Hin_dict))\n",
    "            predictions = torch.cat(tuple(x for x in predictions),dim =1).to(device)\n",
    "            total_acc += accuracy(test_hin_word[x:x+batchsize].to(device),predictions,x)\n",
    "            df = translate(test_hin_word[x:x+batchsize],predictions,df)\n",
    "            loss  = nn.CrossEntropyLoss(reduction = 'sum')(predicted,test_hin_word[x:x+batchsize].reshape(-1).to(device))\n",
    "            with torch.no_grad():\n",
    "                total_loss += loss.item()\n",
    "        test_loss = total_loss/(len(testdata)*test_max_hin_len)\n",
    "        test_accuracy = (total_acc/len(testdata))*100\n",
    "        del(predictions)\n",
    "        del(predicted)\n",
    "        del(input2)\n",
    "        del(output1)\n",
    "        del(output2)\n",
    "        del(output3)\n",
    "        del(hidden1)\n",
    "        del(cell1)\n",
    "        del(OGhidden)\n",
    "        del(output)\n",
    "        del(cell)\n",
    "        return test_loss,test_accuracy,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
